{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAh9mnITdgwg"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JX4elvPodjJB"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Callable, Optional\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image,  ImageOps\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torch.utils.data import DataLoader \n",
    "from torchvision import transforms\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ai7qf1oAsoCa",
    "outputId": "de7a7ae9-8c9c-4ea8-abb6-cb9b6cfbb091"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPSvPqkVPES9"
   },
   "outputs": [],
   "source": [
    "path_to_dataset = '/content/drive/MyDrive/datasets/masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jge_6zzJDdow"
   },
   "outputs": [],
   "source": [
    "path_to_exp = '/content/drive/MyDrive/Froth/froth_dataset/exp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ow8sv1fJO-lv"
   },
   "source": [
    "# Preparing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_AObI00Wh0yr",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "67b5a0a3-ffab-4e10-dcaf-f2d8290757cc"
   },
   "outputs": [],
   "source": [
    "!mkdir /content/drive/MyDrive/Froth/froth_dataset/exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3i7Zu1vFm7_3"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Author: Manpreet Singh Minhas\n",
    "Contact: msminhas at uwaterloo ca\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class SegmentationDataset(VisionDataset):\n",
    "    \"\"\"A PyTorch dataset for image segmentation task.\n",
    "    The dataset is compatible with torchvision transforms.\n",
    "    The transforms passed would be applied to both the Images and Masks.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 root: str,\n",
    "                 image_folder: str,\n",
    "                 mask_folder: str,\n",
    "                 transforms: Optional[Callable] = None,\n",
    "                 seed: int = None,\n",
    "                 fraction: float = None,\n",
    "                 subset: str = None,\n",
    "                 image_color_mode: str = \"rgb\",\n",
    "                 mask_color_mode: str = \"grayscale\") -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (str): Root directory path.\n",
    "            image_folder (str): Name of the folder that contains the images in the root directory.\n",
    "            mask_folder (str): Name of the folder that contains the masks in the root directory.\n",
    "            transforms (Optional[Callable], optional): A function/transform that takes in\n",
    "            a sample and returns a transformed version.\n",
    "            E.g, ``transforms.ToTensor`` for images. Defaults to None.\n",
    "            seed (int, optional): Specify a seed for the train and test split for reproducible results. Defaults to None.\n",
    "            fraction (float, optional): A float value from 0 to 1 which specifies the validation split fraction. Defaults to None.\n",
    "            subset (str, optional): 'Train' or 'Test' to select the appropriate set. Defaults to None.\n",
    "            image_color_mode (str, optional): 'rgb' or 'grayscale'. Defaults to 'rgb'.\n",
    "            mask_color_mode (str, optional): 'rgb' or 'grayscale'. Defaults to 'grayscale'.\n",
    "        Raises:\n",
    "            OSError: If image folder doesn't exist in root.\n",
    "            OSError: If mask folder doesn't exist in root.\n",
    "            ValueError: If subset is not either 'Train' or 'Test'\n",
    "            ValueError: If image_color_mode and mask_color_mode are either 'rgb' or 'grayscale'\n",
    "        \"\"\"\n",
    "        super().__init__(root, transforms)\n",
    "        image_folder_path = Path(self.root) / image_folder\n",
    "        mask_folder_path = Path(self.root) / mask_folder\n",
    "        if not image_folder_path.exists():\n",
    "            raise OSError(f\"{image_folder_path} does not exist.\")\n",
    "        if not mask_folder_path.exists():\n",
    "            raise OSError(f\"{mask_folder_path} does not exist.\")\n",
    "\n",
    "        if image_color_mode not in [\"rgb\", \"grayscale\"]:\n",
    "            raise ValueError(\n",
    "                f\"{image_color_mode} is an invalid choice. Please enter from rgb grayscale.\"\n",
    "            )\n",
    "        if mask_color_mode not in [\"rgb\", \"grayscale\"]:\n",
    "            raise ValueError(\n",
    "                f\"{mask_color_mode} is an invalid choice. Please enter from rgb grayscale.\"\n",
    "            )\n",
    "\n",
    "        self.image_color_mode = image_color_mode\n",
    "        self.mask_color_mode = mask_color_mode\n",
    "\n",
    "        if not fraction:\n",
    "            self.image_names = sorted(image_folder_path.glob(\"*\"))\n",
    "            self.mask_names = sorted(mask_folder_path.glob(\"*\"))\n",
    "        else:\n",
    "            if subset not in [\"Train\", \"Test\"]:\n",
    "                raise (ValueError(\n",
    "                    f\"{subset} is not a valid input. Acceptable values are Train and Test.\"\n",
    "                ))\n",
    "            self.fraction = fraction\n",
    "            self.image_list = np.array(sorted(image_folder_path.glob(\"*\")))\n",
    "            self.mask_list = np.array(sorted(mask_folder_path.glob(\"*\")))\n",
    "            if seed:\n",
    "                np.random.seed(seed)\n",
    "                indices = np.arange(len(self.image_list))\n",
    "                np.random.shuffle(indices)\n",
    "                self.image_list = self.image_list[indices]\n",
    "                self.mask_list = self.mask_list[indices]\n",
    "            if subset == \"Train\":\n",
    "                self.image_names = self.image_list[:int(\n",
    "                    np.ceil(len(self.image_list) * (1 - self.fraction)))]\n",
    "                self.mask_names = self.mask_list[:int(\n",
    "                    np.ceil(len(self.mask_list) * (1 - self.fraction)))]\n",
    "            else:\n",
    "                self.image_names = self.image_list[\n",
    "                    int(np.ceil(len(self.image_list) * (1 - self.fraction))):]\n",
    "                self.mask_names = self.mask_list[\n",
    "                    int(np.ceil(len(self.mask_list) * (1 - self.fraction))):]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        image_path = self.image_names[index]\n",
    "        mask_path = self.mask_names[index]\n",
    "        with open(image_path, \"rb\") as image_file, open(mask_path,\n",
    "                                                        \"rb\") as mask_file:\n",
    "            image = Image.open(image_file)\n",
    "            w, h = image.size\n",
    "            image = ImageOps.autocontrast(image)\n",
    "            # image = ImageOps.equalize(image)\n",
    "            image = image.crop((0, 0,h, h))\n",
    "            if self.image_color_mode == \"rgb\":\n",
    "                image = image.convert(\"RGB\")\n",
    "            elif self.image_color_mode == \"grayscale\":\n",
    "                image = image.convert(\"L\")\n",
    "                # image = image.convert(\"RGB\")\n",
    "            mask = Image.open(mask_file)\n",
    "            mask = mask.crop((0, 0,h, h))\n",
    "            if self.mask_color_mode == \"rgb\":\n",
    "                mask = mask.convert(\"RGB\")\n",
    "            elif self.mask_color_mode == \"grayscale\":\n",
    "                mask = mask.convert(\"L\")\n",
    "                # image = image.convert(\"RGB\")\n",
    "            sample = {\"image\": image, \"mask\": mask}\n",
    "            if self.transforms:\n",
    "                sample[\"image\"] = self.transforms(sample[\"image\"])\n",
    "                sample[\"mask\"] = self.transforms(sample[\"mask\"])\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9nct3QDBjcM"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_dataloader_single_folder(data_dir: str,\n",
    "                                 image_folder: str = 'Images',\n",
    "                                 mask_folder: str = 'Masks',\n",
    "                                 fraction: float = 0.2,\n",
    "                                 batch_size: int = 8):\n",
    "    \"\"\"Create train and test dataloader from a single directory containing\n",
    "    the image and mask folders.\n",
    "    Args:\n",
    "        data_dir (str): Data directory path or root\n",
    "        image_folder (str, optional): Image folder name. Defaults to 'Images'.\n",
    "        mask_folder (str, optional): Mask folder name. Defaults to 'Masks'.\n",
    "        fraction (float, optional): Fraction of Test set. Defaults to 0.2.\n",
    "        batch_size (int, optional): Dataloader batch size. Defaults to 4.\n",
    "    Returns:\n",
    "        dataloaders: Returns dataloaders dictionary containing the\n",
    "        Train and Test dataloaders.\n",
    "    \"\"\"\n",
    "    # data_transforms = transforms.Compose([transforms.RandomCrop(size=(256, 256)), transforms.ToTensor()])\n",
    "    data_transforms = transforms.Compose([\n",
    "                                          transforms.Resize(size=(256, 256)), \n",
    "                                          transforms.Grayscale(num_output_channels=1),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          ])\n",
    "\n",
    "    image_datasets = {\n",
    "        x: SegmentationDataset(data_dir,\n",
    "                               image_folder=image_folder,\n",
    "                               mask_folder=mask_folder,\n",
    "                               seed=100,\n",
    "                               fraction=fraction,\n",
    "                               subset=x,\n",
    "                               transforms=data_transforms)\n",
    "        for x in ['Train', 'Test']\n",
    "    }\n",
    "    dataloaders = {\n",
    "        x: DataLoader(image_datasets[x],\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      num_workers=2)\n",
    "        for x in ['Train', 'Test']\n",
    "    }\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNelvm5poz9j"
   },
   "source": [
    "# W&B init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKLT1TPfQy1f",
    "outputId": "426e89c7-9b22-4c58-bae7-b6d1abc718a4"
   },
   "outputs": [],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jgjwnmwpSjn"
   },
   "outputs": [],
   "source": [
    "# Flexible integration for any Python script\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42nQPLsbQwtN"
   },
   "outputs": [],
   "source": [
    "def update_wb(model_name, lr, loss, th, opt):\n",
    "  wandb.init(project=\"froth\", entity=\"s70c3\")\n",
    "  wandb.run.name = f'{model_name}_lr{str(lr)}_loss_{loss}_th_{th}_opt_{opt}'\n",
    "  # 2. Save model inputs and hyperparameters\n",
    "  config = wandb.config\n",
    "  config.learning_rate = lr\n",
    "  config.epoch = 50\n",
    "  config.batch_size=8\n",
    "  config.name = model_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOwG0r7sYChF"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDvU7DCeR2uU"
   },
   "outputs": [],
   "source": [
    "SMOOTH = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ZDIoD1yAyRl"
   },
   "outputs": [],
   "source": [
    "def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n",
    "    # You can comment out this line if you are passing tensors of equal shape\n",
    "    # But if you are passing output from UNet or something it will most probably\n",
    "    # be with the BATCH x 1 x H x W shape\n",
    "    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n",
    "    \n",
    "    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
    "    union = (outputs | labels).float().sum((1, 2))         # Will be zzero if both are 0\n",
    "    \n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n",
    "    \n",
    "    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n",
    "    \n",
    "    return thresholded  # Or thresholded.mean() if you are interested in average across the batch\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8A9_t2a4YFly"
   },
   "outputs": [],
   "source": [
    "def iou(outputs: np.array, labels: np.array):\n",
    "    intersection = (outputs & labels).sum()\n",
    "    union = (outputs | labels).sum()\n",
    "    \n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)\n",
    "    \n",
    "    thresholded = np.ceil(np.clip(20 * (iou - 0.5), 0, 10)) / 10\n",
    "    \n",
    "    return iou # Or thresholded.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EcCy7onb7xq"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def dice_loss(pred, target):\n",
    "    pred = torch.sigmoid(pred)\n",
    "\n",
    "    pred = pred.contiguous().view(-1)\n",
    "    target = target.contiguous().view(-1)\n",
    "\n",
    "    intersection = torch.sum(pred * target)\n",
    "    pred_sum = torch.sum(pred * pred)\n",
    "    target_sum = torch.sum(target * target)\n",
    "\n",
    "    return 1 - ((2. * intersection + 1e-5) / (pred_sum + target_sum + 1e-5))\n"
   ],
   "metadata": {
    "id": "NG_CxaTEWy1k"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0ASJq5aPK-A"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, criterion, dataloaders, optimizer, metrics, bpath,\n",
    "                num_epochs, needs_out=False, threshold=0.5, scheduler=None):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "    # Use gpu if available\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    # Initialize the log file for training and testing loss and metrics\n",
    "    fieldnames = ['epoch', 'Train_loss', 'Test_loss', '_timestamp', '_runtime'] + \\\n",
    "        [f'Train_{m}' for m in metrics.keys()] + \\\n",
    "        [f'Test_{m}' for m in metrics.keys()]\n",
    "    with open(os.path.join(bpath, 'log.csv'), 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "        # Each epoch has a training and validation phase\n",
    "        # Initialize batch summary\n",
    "        batchsummary = {a: [0] for a in fieldnames}\n",
    "\n",
    "        for phase in ['Train', 'Test']:\n",
    "            if phase == 'Train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            # Iterate over data.\n",
    "            for sample in tqdm(iter(dataloaders[phase])):\n",
    "                inputs = sample['image'].to(device)\n",
    "                masks = sample['mask'].to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'Train'):\n",
    "               \n",
    "                    outputs = model(inputs)\n",
    "                    # print(outputs)\n",
    "                    if needs_out:\n",
    "                      loss = criterion(outputs['out'], masks)\n",
    "                      y_pred = outputs['out'].data.cpu().numpy().ravel()\n",
    "                    else:\n",
    "                      loss = criterion(outputs, masks)\n",
    "                      y_pred = outputs.data.cpu().numpy().ravel()\n",
    "                    y_true = masks.data.cpu().numpy().ravel()\n",
    "                    for name, metric in metrics.items():\n",
    "                        if name == 'f1_score':\n",
    "                            # Use a classification threshold of 0.3\n",
    "                            batchsummary[f'{phase}_{name}'].append(\n",
    "                                metric(y_true > 0, y_pred > threshold))\n",
    "                        elif name == 'iou':\n",
    "                          batchsummary[f'{phase}_{name}'].append(\n",
    "                                metric(y_true > 0, y_pred > threshold))\n",
    "                        else:\n",
    "                            batchsummary[f'{phase}_{name}'].append(\n",
    "                                metric(y_true.astype('uint8'), y_pred))\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'Train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "            # if scheduler is not None:\n",
    "            #   scheduler.step()\n",
    "            batchsummary['epoch'] = epoch\n",
    "            epoch_loss = loss\n",
    "            batchsummary[f'{phase}_loss'] = epoch_loss.item()\n",
    "            # print('{} Loss: {:.4f}'.format(phase, loss))\n",
    "        for field in fieldnames[3:]:\n",
    "            batchsummary[field] = np.mean(batchsummary[field])\n",
    "        \n",
    "        wandb.log(batchsummary)\n",
    "        with open(os.path.join(bpath, 'log.csv'), 'a', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            print(batchsummary)\n",
    "            writer.writerow(batchsummary)\n",
    "            # deep copy the model\n",
    "            if phase == 'Test' and loss < best_loss:\n",
    "                best_loss = loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Lowest Loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gRdBdV-A6S7"
   },
   "source": [
    "# Ready to train & eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ST3vkYXEbEAS"
   },
   "outputs": [],
   "source": [
    "#PyTorch\n",
    "class DiceLoss(torch.nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = torch.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3eUxOnxA_0s"
   },
   "outputs": [],
   "source": [
    "def main(data_directory, exp_directory, epochs, batch_size, model, model_name, lr=1e-4, needs_out=False, threshold=0.5):\n",
    "    # Create the deeplabv3 resnet101 model which is pretrained on a subset\n",
    "    # of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    data_directory = Path(data_directory)\n",
    "    # Create the experiment directory if not present\n",
    "    exp_directory = Path(exp_directory)\n",
    "    if not exp_directory.exists():\n",
    "        exp_directory.mkdir()\n",
    "\n",
    "    # Specify the loss function\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    # criterion = DiceLoss()\n",
    "    # criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Specify the optimizer with a lower learning rate\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs//5, lr/10)\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "\n",
    "    # Specify the evaluation metrics\n",
    "    metrics = {'f1_score': f1_score, 'iou': jaccard_score}\n",
    "\n",
    "    # Create the dataloader\n",
    "    dataloaders = get_dataloader_single_folder(\n",
    "        data_directory, 'image', 'label', batch_size=batch_size)\n",
    "    \n",
    "    samples = next(iter(dataloaders['Train']))\n",
    "# Display the image and mask tensor shape\n",
    "# We see the tensor size is correct bxcxhxw, where b is batch size, c is number of channels, h is height, w is width\n",
    "    wandb.watch(model)\n",
    "    _ = train_model(model,\n",
    "                      criterion,\n",
    "                      dataloaders,\n",
    "                      optimizer,\n",
    "                      bpath=exp_directory,\n",
    "                      metrics=metrics,\n",
    "                      num_epochs=epochs, \n",
    "                      needs_out=needs_out,\n",
    "                      threshold=threshold,\n",
    "                      scheduler = scheduler\n",
    "                    )\n",
    "\n",
    "      # Save the trained model\n",
    "    torch.save(model, exp_directory / f'{model_name}_weights.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPSc2383owWs"
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-l8-q8ZID_In"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score, roc_auc_score, jaccard_score\n",
    "from torch.utils import data\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_DkdXswqorB"
   },
   "source": [
    "## TransUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xyC9gV8EqyZR",
    "outputId": "96aeb6a7-4a39-4583-a163-91db964bb564"
   },
   "outputs": [],
   "source": [
    "pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6PwtedtrAFB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_num = head_num\n",
    "        self.dk = (embedding_dim // head_num) ** (1 / 2)\n",
    "\n",
    "        self.qkv_layer = nn.Linear(embedding_dim, embedding_dim * 3, bias=False)\n",
    "        self.out_attention = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        qkv = self.qkv_layer(x)\n",
    "\n",
    "        query, key, value = tuple(rearrange(qkv, 'b t (d k h ) -> k b h t d ', k=3, h=self.head_num))\n",
    "        energy = torch.einsum(\"... i d , ... j d -> ... i j\", query, key) * self.dk\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask, -np.inf)\n",
    "\n",
    "        attention = torch.softmax(energy, dim=-1)\n",
    "\n",
    "        x = torch.einsum(\"... i j , ... j d -> ... i d\", attention, value)\n",
    "\n",
    "        x = rearrange(x, \"b h t d -> b t (h d)\")\n",
    "        x = self.out_attention(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, embedding_dim, mlp_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp_layers = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(mlp_dim, embedding_dim),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mlp_layers(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_num, mlp_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(embedding_dim, head_num)\n",
    "        self.mlp = MLP(embedding_dim, mlp_dim)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _x = self.multi_head_attention(x)\n",
    "        _x = self.dropout(_x)\n",
    "        x = x + _x\n",
    "        x = self.layer_norm1(x)\n",
    "\n",
    "        _x = self.mlp(x)\n",
    "        x = x + _x\n",
    "        x = self.layer_norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, head_num, mlp_dim, block_num=12):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_blocks = nn.ModuleList(\n",
    "            [TransformerEncoderBlock(embedding_dim, head_num, mlp_dim) for _ in range(block_num)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer_block in self.layer_blocks:\n",
    "            x = layer_block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_dim, in_channels, embedding_dim, head_num, mlp_dim,\n",
    "                 block_num, patch_dim, classification=True, num_classes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_dim = patch_dim\n",
    "        self.classification = classification\n",
    "        self.num_tokens = (img_dim // patch_dim) ** 2\n",
    "        self.token_dim = in_channels * (patch_dim ** 2)\n",
    "\n",
    "        self.projection = nn.Linear(self.token_dim, embedding_dim)\n",
    "        self.embedding = nn.Parameter(torch.rand(self.num_tokens + 1, embedding_dim))\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.transformer = TransformerEncoder(embedding_dim, head_num, mlp_dim, block_num)\n",
    "\n",
    "        if self.classification:\n",
    "            self.mlp_head = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        img_patches = rearrange(x,\n",
    "                                'b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)',\n",
    "                                patch_x=self.patch_dim, patch_y=self.patch_dim)\n",
    "\n",
    "        batch_size, tokens, _ = img_patches.shape\n",
    "\n",
    "        project = self.projection(img_patches)\n",
    "        token = repeat(self.cls_token, 'b ... -> (b batch_size) ...',\n",
    "                       batch_size=batch_size)\n",
    "\n",
    "        patches = torch.cat([token, project], dim=1)\n",
    "        patches += self.embedding[:tokens + 1, :]\n",
    "\n",
    "        x = self.dropout(patches)\n",
    "        x = self.transformer(x)\n",
    "        x = self.mlp_head(x[:, 0, :]) if self.classification else x[:, 1:, :]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MZbSdEEIqqoY",
    "outputId": "574e3878-da32-4b8e-96f1-4c88d38c19e8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "\n",
    "class EncoderBottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, base_width=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.downsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        width = int(out_channels * (base_width / 64))\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1, bias=False)\n",
    "        self.norm1 = nn.BatchNorm2d(width)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=2, groups=1, padding=1, dilation=1, bias=False)\n",
    "        self.norm2 = nn.BatchNorm2d(width)\n",
    "        # self.conv2 = nn.Conv2d(out_channels * 8, out_channels * 4, kernel_size=3, stride=1, padding=1)\n",
    "        # self.norm2 = nn.BatchNorm2d(out_channels * 4)   \n",
    "\n",
    "        self.conv3 = nn.Conv2d(width, out_channels, kernel_size=1, stride=1, bias=False)\n",
    "        self.norm3 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_down = self.downsample(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = x + x_down\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, scale_factor=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=scale_factor, mode='bilinear', align_corners=True)\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_concat=None):\n",
    "        x = self.upsample(x)\n",
    "\n",
    "        if x_concat is not None:\n",
    "            x = torch.cat([x_concat, x], dim=1)\n",
    "\n",
    "        x = self.layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, img_dim, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.norm1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.encoder1 = EncoderBottleneck(out_channels, out_channels * 2, stride=2)\n",
    "        self.encoder2 = EncoderBottleneck(out_channels * 2, out_channels * 4, stride=2)\n",
    "        self.encoder3 = EncoderBottleneck(out_channels * 4, out_channels * 8, stride=2)\n",
    "\n",
    "        self.vit_img_dim = img_dim // patch_dim\n",
    "        self.vit = ViT(self.vit_img_dim, out_channels * 8, out_channels * 8,\n",
    "                       head_num, mlp_dim, block_num, patch_dim=1, classification=False)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels * 8, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.norm2 = nn.BatchNorm2d(512)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x1 = self.relu(x)\n",
    "\n",
    "        x2 = self.encoder1(x1)\n",
    "        x3 = self.encoder2(x2)\n",
    "        x = self.encoder3(x3)\n",
    "\n",
    "        x = self.vit(x)\n",
    "        x = rearrange(x, \"b (x y) c -> b c x y\", x=self.vit_img_dim, y=self.vit_img_dim)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x, x1, x2, x3\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_channels, class_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder1 = DecoderBottleneck(out_channels * 8, out_channels * 2)\n",
    "        self.decoder2 = DecoderBottleneck(out_channels * 4, out_channels)\n",
    "        self.decoder3 = DecoderBottleneck(out_channels * 2, int(out_channels * 1 / 2))\n",
    "        self.decoder4 = DecoderBottleneck(int(out_channels * 1 / 2), int(out_channels * 1 / 8))\n",
    "\n",
    "        self.conv1 = nn.Conv2d(int(out_channels * 1 / 8), class_num, kernel_size=1)\n",
    "\n",
    "        self.active = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, x1, x2, x3):\n",
    "        x = self.decoder1(x, x3)\n",
    "        x = self.decoder2(x, x2)\n",
    "        x = self.decoder3(x, x1)\n",
    "        x = self.decoder4(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.active(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransUNet(nn.Module):\n",
    "    def __init__(self, img_dim, in_channels, out_channels, head_num, mlp_dim, block_num, patch_dim, class_num):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(img_dim, in_channels, out_channels,\n",
    "                               head_num, mlp_dim, block_num, patch_dim)\n",
    "\n",
    "        self.decoder = Decoder(out_channels, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, x1, x2, x3 = self.encoder(x)\n",
    "        x = self.decoder(x, x1, x2, x3)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import torch\n",
    "\n",
    "    transunet = TransUNet(img_dim=128,\n",
    "                          in_channels=3,\n",
    "                          out_channels=128,\n",
    "                          head_num=4,\n",
    "                          mlp_dim=512,\n",
    "                          block_num=8,\n",
    "                          patch_dim=16,\n",
    "                          class_num=1)\n",
    "\n",
    "    print(sum(p.numel() for p in transunet.parameters()))\n",
    "    print(transunet(torch.randn(1, 3, 128, 128)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPF2RrxDrLvT"
   },
   "outputs": [],
   "source": [
    "def createTransUnet():\n",
    "    # model = TransUNet(img_dim=512,\n",
    "    #                     in_channels=3,\n",
    "    #                     out_channels=128,\n",
    "    #                     head_num=4,\n",
    "    #                     mlp_dim=512,\n",
    "    #                     block_num=8,\n",
    "    #                     patch_dim=16,\n",
    "    #                     class_num=1)\n",
    "    model = TransUNet(img_dim=256,\n",
    "                    in_channels=1,\n",
    "                    out_channels=128,\n",
    "                    head_num=4,\n",
    "                    mlp_dim=512,\n",
    "                    block_num=8,\n",
    "                    patch_dim=16,\n",
    "                    class_num=1)\n",
    "    model.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jP54pcfTrYDN"
   },
   "outputs": [],
   "source": [
    "#https://github.com/mkara44/transunet_pytorch/tree/929e07b35a9769a7deffc42031322fe722545190"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaccB2f9nPiE"
   },
   "source": [
    "## AttUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnjGUdYOnRL-"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torch\n",
    "\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolution Block \n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(conv_block, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    \"\"\"\n",
    "    Up Convolution Block\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(up_conv, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "        \n",
    "class Attention_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(Attention_block, self).__init__()\n",
    "\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, g, x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        out = x * psi\n",
    "        return out\n",
    "\n",
    "class AttU_Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Unet implementation\n",
    "    Paper: https://arxiv.org/abs/1804.03999\n",
    "    \"\"\"\n",
    "    def __init__(self, img_ch=3, output_ch=1):\n",
    "        super(AttU_Net, self).__init__()\n",
    "\n",
    "        n1 = 64\n",
    "        filters = [n1, n1 * 2, n1 * 4, n1 * 8, n1 * 16]\n",
    "\n",
    "        self.Maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.Maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(img_ch, filters[0])\n",
    "        self.Conv2 = conv_block(filters[0], filters[1])\n",
    "        self.Conv3 = conv_block(filters[1], filters[2])\n",
    "        self.Conv4 = conv_block(filters[2], filters[3])\n",
    "        self.Conv5 = conv_block(filters[3], filters[4])\n",
    "\n",
    "        self.Up5 = up_conv(filters[4], filters[3])\n",
    "        self.Att5 = Attention_block(F_g=filters[3], F_l=filters[3], F_int=filters[2])\n",
    "        self.Up_conv5 = conv_block(filters[4], filters[3])\n",
    "\n",
    "        self.Up4 = up_conv(filters[3], filters[2])\n",
    "        self.Att4 = Attention_block(F_g=filters[2], F_l=filters[2], F_int=filters[1])\n",
    "        self.Up_conv4 = conv_block(filters[3], filters[2])\n",
    "\n",
    "        self.Up3 = up_conv(filters[2], filters[1])\n",
    "        self.Att3 = Attention_block(F_g=filters[1], F_l=filters[1], F_int=filters[0])\n",
    "        self.Up_conv3 = conv_block(filters[2], filters[1])\n",
    "\n",
    "        self.Up2 = up_conv(filters[1], filters[0])\n",
    "        self.Att2 = Attention_block(F_g=filters[0], F_l=filters[0], F_int=32)\n",
    "        self.Up_conv2 = conv_block(filters[1], filters[0])\n",
    "\n",
    "        self.Conv = nn.Conv2d(filters[0], output_ch, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.active = torch.nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        e1 = self.Conv1(x)\n",
    "\n",
    "        e2 = self.Maxpool1(e1)\n",
    "        e2 = self.Conv2(e2)\n",
    "\n",
    "        e3 = self.Maxpool2(e2)\n",
    "        e3 = self.Conv3(e3)\n",
    "\n",
    "        e4 = self.Maxpool3(e3)\n",
    "        e4 = self.Conv4(e4)\n",
    "\n",
    "        e5 = self.Maxpool4(e4)\n",
    "        e5 = self.Conv5(e5)\n",
    "\n",
    "        #print(x5.shape)\n",
    "        d5 = self.Up5(e5)\n",
    "        #print(d5.shape)\n",
    "        x4 = self.Att5(g=d5, x=e4)\n",
    "        d5 = torch.cat((x4, d5), dim=1)\n",
    "        d5 = self.Up_conv5(d5)\n",
    "\n",
    "        d4 = self.Up4(d5)\n",
    "        x3 = self.Att4(g=d4, x=e3)\n",
    "        d4 = torch.cat((x3, d4), dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        x2 = self.Att3(g=d3, x=e2)\n",
    "        d3 = torch.cat((x2, d3), dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        x1 = self.Att2(g=d2, x=e1)\n",
    "        d2 = torch.cat((x1, d2), dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        out = self.Conv(d2)\n",
    "\n",
    "        out = self.active(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFMSixZ0nnnz"
   },
   "outputs": [],
   "source": [
    "def createUnetAtt():\n",
    "    model = AttU_Net(img_ch=1)\n",
    "    model.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "vsBAZbKKQsMj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DS-TransUnet"
   ],
   "metadata": {
    "id": "KXecd9AkCLgh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pip install timm"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hWuzcwfAhoGW",
    "outputId": "c8f86db0-71c9-45d1-d11c-6f24c53f6146"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_k = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(dim, inner_dim, bias=False)\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x_qkv):\n",
    "        b, n, _, h = *x_qkv.shape, self.heads\n",
    "\n",
    "        k = self.to_k(x_qkv)\n",
    "        k = rearrange(k, 'b n (h d) -> b h n d', h=h)\n",
    "\n",
    "        v = self.to_v(x_qkv)\n",
    "        v = rearrange(v, 'b n (h d) -> b h n d', h=h)\n",
    "\n",
    "        q = self.to_q(x_qkv[:, 0].unsqueeze(1))\n",
    "        q = rearrange(q, 'b n (h d) -> b h n d', h=h)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "import numpy as np\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from utils.checkpoint import load_checkpoint\n",
    "# from mmseg.utils import get_root_logger\n",
    "\n",
    "# from utils.module import Attention, PreNorm, FeedForward, CrossAttention\n",
    "\n",
    "groups = 32\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\" Multilayer perceptron.\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features).cuda()\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features).cuda()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n",
    "    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n",
    "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    \"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size[0])\n",
    "        coords_w = torch.arange(self.window_size[1])\n",
    "        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\" Forward function.\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, N, N)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    \"\"\" Swin Transformer Block.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, num_heads, window_size=7, shift_size=0,\n",
    "                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        self.H = None\n",
    "        self.W = None\n",
    "\n",
    "    def forward(self, x, mask_matrix):\n",
    "        \"\"\" Forward function.\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "            mask_matrix: Attention mask for cyclic shift.\n",
    "        \"\"\"\n",
    "        B, L, C = x.shape\n",
    "        H, W = self.H, self.W\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # pad feature maps to multiples of window size\n",
    "        pad_l = pad_t = 0\n",
    "        pad_r = (self.window_size - W % self.window_size) % self.window_size\n",
    "        pad_b = (self.window_size - H % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "        _, Hp, Wp, _ = x.shape\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n",
    "            attn_mask = mask_matrix\n",
    "        else:\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, Hp, Wp)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        if pad_r > 0 or pad_b > 0:\n",
    "            x = x[:, :H, :W, :].contiguous()\n",
    "\n",
    "        x = x.view(B, H * W, C)\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchRecover(nn.Module):\n",
    "    \"\"\" Patch Merging Layer\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(dim, dim // 2, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.GroupNorm(num_channels=dim // 2, num_groups=groups),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\" Forward function.\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.permute(0, 1, 2)  # B ,C, L\n",
    "        x = x.reshape(B, C, H, W)\n",
    "        x = self.up(x)  # B, C//2, H, W\n",
    "\n",
    "        x = x.reshape(B, C // 2, -1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        # x = Variable(torch.randn(B, H * 2, W * 2, C // 2))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Module):\n",
    "    \"\"\" Patch Merging Layer\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\" Forward function.\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "\n",
    "        # padding\n",
    "        pad_input = (H % 2 == 1) or (W % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "    Args:\n",
    "        dim (int): Number of feature channels\n",
    "        depth (int): Depths of this stage.\n",
    "        num_heads (int): Number of attention head.\n",
    "        window_size (int): Local window size. Default: 7.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 depth,\n",
    "                 num_heads,\n",
    "                 window_size=7,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 downsample=None,\n",
    "                 use_checkpoint=False,\n",
    "                 up=True):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = window_size // 2\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.up = up\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(\n",
    "                dim=dim,\n",
    "                num_heads=num_heads,\n",
    "                window_size=window_size,\n",
    "                shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop,\n",
    "                attn_drop=attn_drop,\n",
    "                drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                norm_layer=norm_layer)\n",
    "            for i in range(depth)])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        \"\"\" Forward function.\n",
    "        Args:\n",
    "            x: Input feature, tensor size (B, H*W, C).\n",
    "            H, W: Spatial resolution of the input feature.\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate attention mask for SW-MSA\n",
    "        Hp = int(np.ceil(H / self.window_size)) * self.window_size\n",
    "        Wp = int(np.ceil(W / self.window_size)) * self.window_size\n",
    "        img_mask = torch.zeros((1, Hp, Wp, 1), device=x.device)  # 1 Hp Wp 1\n",
    "        h_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size),\n",
    "                    slice(-self.window_size, -self.shift_size),\n",
    "                    slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            blk.H, blk.W = H, W\n",
    "            if self.use_checkpoint:\n",
    "                x = checkpoint.checkpoint(blk, x, attn_mask)\n",
    "            else:\n",
    "                x = blk(x, attn_mask)\n",
    "        if self.downsample is not None:\n",
    "            x_down = self.downsample(x, H, W)\n",
    "            if self.up:\n",
    "                Wh, Ww = (H + 1) // 2, (W + 1) // 2\n",
    "            else:\n",
    "                Wh, Ww = H * 2, W * 2\n",
    "            return x, H, W, x_down, Wh, Ww\n",
    "        else:\n",
    "            return x, H, W, x, H, W\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        # padding\n",
    "        _, _, H, W = x.size()\n",
    "        if W % self.patch_size[1] != 0:\n",
    "            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n",
    "        if H % self.patch_size[0] != 0:\n",
    "            x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n",
    "\n",
    "        x = self.proj(x)  # B C Wh Ww\n",
    "        if self.norm is not None:\n",
    "            Wh, Ww = x.size(2), x.size(3)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "            x = self.norm(x)\n",
    "            x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiEmbed(nn.Module):\n",
    "\n",
    "    def __init__(self, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=1)\n",
    "        self.bn = nn.BatchNorm2d(embed_dim)\n",
    "        self.maxPool = nn.MaxPool2d(kernel_size=patch_size, stride=patch_size)\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        # padding\n",
    "        _, _, H, W = x.size()\n",
    "        if W % self.patch_size[1] != 0:\n",
    "            x = F.pad(x, (0, self.patch_size[1] - W % self.patch_size[1]))\n",
    "        if H % self.patch_size[0] != 0:\n",
    "            x = F.pad(x, (0, 0, 0, self.patch_size[0] - H % self.patch_size[0]))\n",
    "\n",
    "        x = self.proj(x)  # B C Wh Ww\n",
    "        x = self.bn(x)\n",
    "        x = self.maxPool(x)\n",
    "        if self.norm is not None:\n",
    "            Wh, Ww = x.size(2), x.size(3)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "            x = self.norm(x)\n",
    "            x = x.transpose(1, 2).view(-1, self.embed_dim, Wh, Ww)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    \"\"\" Swin Transformer backbone.\n",
    "        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n",
    "          https://arxiv.org/pdf/2103.14030\n",
    "    Args:\n",
    "        pretrain_img_size (int): Input image size for training the pretrained model,\n",
    "            used in absolute postion embedding. Default 224.\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        depths (tuple[int]): Depths of each Swin Transformer stage.\n",
    "        num_heads (tuple[int]): Number of attention head of each stage.\n",
    "        window_size (int): Window size. Default: 7.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop_rate (float): Dropout rate.\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0.\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.2.\n",
    "        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False.\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True.\n",
    "        out_indices (Sequence[int]): Output from which stages.\n",
    "        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n",
    "            -1 means not freezing any parameters.\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrain_img_size=224,\n",
    "                 patch_size=4,\n",
    "                 in_chans=3,\n",
    "                 embed_dim=128,\n",
    "                 depths=[2, 2, 18, 2],\n",
    "                 num_heads=[4, 8, 16, 32],\n",
    "                 window_size=7,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.5,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 ape=False,\n",
    "                 patch_norm=True,\n",
    "                 out_indices=(0, 1, 2, 3),\n",
    "                 frozen_stages=-1,\n",
    "                 use_checkpoint=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pretrain_img_size = pretrain_img_size\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.out_indices = out_indices\n",
    "        self.frozen_stages = frozen_stages\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None)\n",
    "\n",
    "        # absolute position embedding\n",
    "        if self.ape:\n",
    "            pretrain_img_size = to_2tuple(pretrain_img_size)\n",
    "            patch_size = to_2tuple(patch_size)\n",
    "            patches_resolution = [pretrain_img_size[0] // patch_size[0], pretrain_img_size[1] // patch_size[1]]\n",
    "\n",
    "            self.absolute_pos_embed = nn.Parameter(\n",
    "                torch.zeros(1, embed_dim, patches_resolution[0], patches_resolution[1]))\n",
    "            trunc_normal_(self.absolute_pos_embed, std=.02)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(\n",
    "                dim=int(embed_dim * 2 ** i_layer),\n",
    "                depth=depths[i_layer],\n",
    "                num_heads=num_heads[i_layer],\n",
    "                window_size=window_size,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,\n",
    "                use_checkpoint=use_checkpoint)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        num_features = [int(embed_dim * 2 ** i) for i in range(self.num_layers)]\n",
    "        self.num_features = num_features\n",
    "\n",
    "        # add a norm layer for each output\n",
    "        for i_layer in out_indices:\n",
    "            layer = norm_layer(num_features[i_layer])\n",
    "            layer_name = f'norm{i_layer}'\n",
    "            self.add_module(layer_name, layer)\n",
    "\n",
    "        self._freeze_stages()\n",
    "\n",
    "    def _freeze_stages(self):\n",
    "        if self.frozen_stages >= 0:\n",
    "            self.patch_embed.eval()\n",
    "            for param in self.patch_embed.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if self.frozen_stages >= 1 and self.ape:\n",
    "            self.absolute_pos_embed.requires_grad = False\n",
    "\n",
    "        if self.frozen_stages >= 2:\n",
    "            self.pos_drop.eval()\n",
    "            for i in range(0, self.frozen_stages - 1):\n",
    "                m = self.layers[i]\n",
    "                m.eval()\n",
    "                for param in m.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def init_weights(self, pretrained=None):\n",
    "        \"\"\"Initialize the weights in backbone.\n",
    "        Args:\n",
    "            pretrained (str, optional): Path to pre-trained weights.\n",
    "                Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        def _init_weights(m):\n",
    "            if isinstance(m, nn.Linear):\n",
    "                trunc_normal_(m.weight, std=.02)\n",
    "                if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "            # if isinstance(pretrained, str):\n",
    "            #     self.apply(_init_weights)\n",
    "            #     logger = get_root_logger()\n",
    "            #     load_checkpoint(self, pretrained, strict=False, logger=logger)\n",
    "            # elif pretrained is None:\n",
    "            self.apply(_init_weights)\n",
    "        # else:\n",
    "        # raise TypeError('pretrained must be a str or None')\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        Wh, Ww = x.size(2), x.size(3)\n",
    "        if self.ape:\n",
    "            # interpolate the position embedding to the corresponding size\n",
    "            absolute_pos_embed = F.interpolate(self.absolute_pos_embed, size=(Wh, Ww), mode='bicubic')\n",
    "            x = (x + absolute_pos_embed).flatten(2).transpose(1, 2)  # B Wh*Ww C\n",
    "        else:\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        outs = []\n",
    "        for i in range(self.num_layers):\n",
    "            layer = self.layers[i]\n",
    "            x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww)\n",
    "\n",
    "            if i in self.out_indices:\n",
    "                norm_layer = getattr(self, f'norm{i}')\n",
    "                x_out = norm_layer(x_out)\n",
    "\n",
    "                out = x_out.view(-1, H, W, self.num_features[i]).permute(0, 3, 1, 2).contiguous()\n",
    "                outs.append(out)\n",
    "\n",
    "        return outs\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        \"\"\"Convert the model into training mode while keep layers freezed.\"\"\"\n",
    "        super(SwinTransformer, self).train(mode)\n",
    "        self._freeze_stages()\n",
    "\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    \"\"\"\n",
    "    Up Convolution Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(up_conv, self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.GroupNorm(num_channels=out_ch, num_groups=groups),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.up = up_conv(in_channels, out_channels)\n",
    "        # self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
    "        self.conv_relu = nn.Sequential(\n",
    "            nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=1),\n",
    "            # coorAtt(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # x2 = self.att_block(x1, x2)\n",
    "        x1 = torch.cat((x2, x1), dim=1)\n",
    "        x1 = self.conv_relu(x1)\n",
    "        return x1\n",
    "\n",
    "\n",
    "class conv_block_ds(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolution Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(conv_block_ds, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.GroupNorm(num_channels=out_ch, num_groups=groups),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.GroupNorm(num_channels=out_ch, num_groups=groups),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Conv_block(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolution Block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.GroupNorm(num_channels=out_ch, num_groups=groups),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.GroupNorm(num_channels=out_ch, num_groups=groups),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinUp(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(SwinUp, self).__init__()\n",
    "        self.up = nn.Linear(dim, dim * 2).cuda()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.up(x)\n",
    "        x = x.reshape(B, H, W, 2 * C)\n",
    "\n",
    "        x0 = x[:, :, :, 0:C // 2]\n",
    "        x1 = x[:, :, :, C // 2:C]\n",
    "        x2 = x[:, :, :, C:C + C // 2]\n",
    "        x3 = x[:, :, :, C + C // 2:C * 2]\n",
    "\n",
    "        x0 = torch.cat((x0, x1), dim=1)\n",
    "        x3 = torch.cat((x2, x3), dim=1)\n",
    "        x = torch.cat((x0, x3), dim=2)\n",
    "\n",
    "        # x = Variable(torch.randn(B, H * 2, W * 2, C // 2))\n",
    "\n",
    "        x = x.reshape(B, -1, C // 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 patch_size=4,\n",
    "                 depths=2,\n",
    "                 num_heads=6,\n",
    "                 window_size=7,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.2,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 patch_norm=True,\n",
    "                 use_checkpoint=False):\n",
    "        super(SwinDecoder, self).__init__()\n",
    "\n",
    "        self.patch_norm = patch_norm\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depths)]  # stochastic depth decay rule\n",
    "\n",
    "        # build layers\n",
    "        self.layer = BasicLayer(\n",
    "            dim=embed_dim // 2,\n",
    "            depth=depths,\n",
    "            num_heads=num_heads,\n",
    "            window_size=window_size,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            drop=drop_rate,\n",
    "            attn_drop=attn_drop_rate,\n",
    "            drop_path=dpr,\n",
    "            norm_layer=norm_layer,\n",
    "            downsample=None,\n",
    "            use_checkpoint=use_checkpoint)\n",
    "\n",
    "        self.up = up_conv(embed_dim, embed_dim // 2)\n",
    "        self.conv_relu = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim // 2, embed_dim // 4, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward function.\"\"\"\n",
    "\n",
    "        # print(x.shape)\n",
    "        # for i in range(len(e_o)):\n",
    "        #    layer = self.layers[i]\n",
    "        #    x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww)\n",
    "        # return x\n",
    "\n",
    "        identity = x\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.up(x)  # B , C//2, 2H, 2W\n",
    "        x = x.reshape(B, C // 2, H * W * 4)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x_out, H, W, x, Wh, Ww = self.layer(x, H * 2, W * 2)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = x.reshape(B, C // 2, H, W)\n",
    "        # B, C//4 2H, 2W\n",
    "        x = self.conv_relu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Swin_Decoder(nn.Module):\n",
    "    def __init__(self, in_channels, depths, num_heads):\n",
    "        super(Swin_Decoder, self).__init__()\n",
    "        self.up = SwinDecoder(in_channels, depths=depths, num_heads=num_heads)\n",
    "        # self.up1 = nn.Upsample(scale_factor=2)\n",
    "        # self.up2 = nn.Conv2d(in_channels, in_channels//2, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "        self.conv_relu = nn.Sequential(\n",
    "            nn.Conv2d(in_channels // 2, in_channels // 2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels // 2, in_channels // 4, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # x1 = self.up2(x1)\n",
    "        # x2 = self.att_block(x1, x2)\n",
    "        x2 = self.conv2(x2)\n",
    "        x1 = torch.cat((x2, x1), dim=1)\n",
    "        out = self.conv_relu(x1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class Cross_Att(nn.Module):\n",
    "    def __init__(self, dim_s, dim_l):\n",
    "        super().__init__()\n",
    "        self.transformer_s = Transformer(dim=dim_s, depth=1, heads=3, dim_head=32, mlp_dim=128)\n",
    "        self.transformer_l = Transformer(dim=dim_l, depth=1, heads=1, dim_head=64, mlp_dim=256)\n",
    "        self.norm_s = nn.LayerNorm(dim_s)\n",
    "        self.norm_l = nn.LayerNorm(dim_l)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.linear_s = nn.Linear(dim_s, dim_l)\n",
    "        self.linear_l = nn.Linear(dim_l, dim_s)\n",
    "\n",
    "    def forward(self, e, r):\n",
    "        b_e, c_e, h_e, w_e = e.shape\n",
    "        e = e.reshape(b_e, c_e, -1).permute(0, 2, 1)\n",
    "        b_r, c_r, h_r, w_r = r.shape\n",
    "        r = r.reshape(b_r, c_r, -1).permute(0, 2, 1)\n",
    "        e_t = torch.flatten(self.avgpool(self.norm_l(e).transpose(1, 2)), 1)\n",
    "        r_t = torch.flatten(self.avgpool(self.norm_s(r).transpose(1, 2)), 1)\n",
    "        e_t = self.linear_l(e_t).unsqueeze(1)\n",
    "        r_t = self.linear_s(r_t).unsqueeze(1)\n",
    "        r = self.transformer_s(torch.cat([e_t, r], dim=1))[:, 1:, :]\n",
    "        e = self.transformer_l(torch.cat([r_t, e], dim=1))[:, 1:, :]\n",
    "        e = e.permute(0, 2, 1).reshape(b_e, c_e, h_e, w_e)\n",
    "        r = r.permute(0, 2, 1).reshape(b_r, c_r, h_r, w_r)\n",
    "        return e, r\n",
    "\n",
    "\n",
    "class DSTransUNet(nn.Module):\n",
    "    def __init__(self, dim, n_class, in_ch=1):\n",
    "        super().__init__()\n",
    "        self.encoder = SwinTransformer(depths=[2, 2, 18, 2], num_heads=[4, 8, 16, 32], drop_path_rate=0.5,\n",
    "                                       embed_dim=128, in_chans=1)\n",
    "        self.encoder2 = SwinTransformer(depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24], drop_path_rate=0.2, patch_size=8,\n",
    "                                        embed_dim=96, in_chans=1)\n",
    "        self.encoder.init_weights('checkpoints/swin_base_patch4_window7_224_22k.pth')\n",
    "        self.encoder2.init_weights('checkpoints/swin_tiny_patch4_window7_224.pth')\n",
    "        self.layer1 = Swin_Decoder(8 * dim, 2, 8)\n",
    "        self.layer2 = Swin_Decoder(4 * dim, 2, 4)\n",
    "        self.layer3 = Swin_Decoder(2 * dim, 2, 2)\n",
    "        self.layer4 = Decoder(dim, dim, dim // 2)\n",
    "        self.layer5 = Decoder(dim // 2, dim // 2, dim // 4)\n",
    "        self.down1 = nn.Conv2d(in_ch, dim // 4, kernel_size=1, stride=1, padding=0)\n",
    "        self.down2 = conv_block_ds(dim // 4, dim // 2)\n",
    "        self.final = nn.Conv2d(dim // 4, n_class, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        self.loss1 = nn.Sequential(\n",
    "            nn.Conv2d(dim * 8, n_class, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=32)\n",
    "        )\n",
    "\n",
    "        self.loss2 = nn.Sequential(\n",
    "            nn.Conv2d(dim, n_class, kernel_size=1, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=4)\n",
    "        )\n",
    "        dim_s = 96\n",
    "        dim_l = 128\n",
    "        self.m1 = nn.Upsample(scale_factor=2)\n",
    "        self.m2 = nn.Upsample(scale_factor=4)\n",
    "        tb = dim_s + dim_l\n",
    "        self.change1 = Conv_block(tb, dim)\n",
    "        self.change2 = Conv_block(tb * 2, dim * 2)\n",
    "        self.change3 = Conv_block(tb * 4, dim * 4)\n",
    "        self.change4 = Conv_block(tb * 8, dim * 8)\n",
    "        self.cross_att_1 = Cross_Att(dim_s * 1, dim_l * 1)\n",
    "        self.cross_att_2 = Cross_Att(dim_s * 2, dim_l * 2)\n",
    "        self.cross_att_3 = Cross_Att(dim_s * 4, dim_l * 4)\n",
    "        self.cross_att_4 = Cross_Att(dim_s * 8, dim_l * 8)\n",
    "        self.active = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out2 = self.encoder2(x)\n",
    "        e1, e2, e3, e4 = out[0], out[1], out[2], out[3]\n",
    "        r1, r2, r3, r4 = out2[0], out2[1], out2[2], out2[3]\n",
    "        e1, r1 = self.cross_att_1(e1, r1)\n",
    "        e2, r2 = self.cross_att_2(e2, r2)\n",
    "        e3, r3 = self.cross_att_3(e3, r3)\n",
    "        e4, r4 = self.cross_att_4(e4, r4)\n",
    "        e1 = torch.cat([e1, self.m1(r1)], 1)\n",
    "        e2 = torch.cat([e2, self.m1(r2)], 1)\n",
    "        e3 = torch.cat([e3, self.m1(r3)], 1)\n",
    "        e4 = torch.cat([e4, self.m1(r4)], 1)\n",
    "        e1 = self.change1(e1)\n",
    "        e2 = self.change2(e2)\n",
    "        e3 = self.change3(e3)\n",
    "        e4 = self.change4(e4)\n",
    "        loss1 = self.loss1(e4)\n",
    "        ds1 = self.down1(x)\n",
    "        ds2 = self.down2(ds1)\n",
    "        d1 = self.layer1(e4, e3)\n",
    "        d2 = self.layer2(d1, e2)\n",
    "        d3 = self.layer3(d2, e1)\n",
    "        loss2 = self.loss2(d3)\n",
    "        d4 = self.layer4(d3, ds2)\n",
    "        d5 = self.layer5(d4, ds1)\n",
    "        o = self.final(d5)\n",
    "        o = self.active(o)\n",
    "\n",
    "        return o\n"
   ],
   "metadata": {
    "id": "kPjzix5ACP3a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def createDSTransUnet():\n",
    "  model = DSTransUNet(256, 1, 1).cuda()\n",
    "  model.train()\n",
    "  return model"
   ],
   "metadata": {
    "id": "ZX_cLP8PLHSu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yxojXiEoQ6t"
   },
   "source": [
    "## Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A88ikND_oSoT"
   },
   "outputs": [],
   "source": [
    "def createUnet(inputchannels=1, outputchannels=1):\n",
    "  model = smp.Unet(\n",
    "      # encoder_name=\"resnet152\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "      # encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "      in_channels=inputchannels,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "      activation = 'sigmoid',\n",
    "      classes=outputchannels,                      # model output channels (number of classes in your dataset)\n",
    "  )\n",
    "  model.train()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWEqgp4hyNcD"
   },
   "source": [
    "## Unet++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CMB32gsRybSn",
    "outputId": "b6f0d1d3-e3af-4ded-da00-da17ef5e1c4f"
   },
   "outputs": [],
   "source": [
    "pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7XLzvMAO1Kae"
   },
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUolbdjuyROI"
   },
   "outputs": [],
   "source": [
    "def createUnetPlusPlus(inputchannels=1, outputchannels=1):\n",
    "  model = smp.UnetPlusPlus(#'resnet152',\n",
    "                          #  encoder_weights=\"imagenet\", \n",
    "                           in_channels=inputchannels, \n",
    "                           activation = 'sigmoid',\n",
    "                           classes=1)\n",
    "  model.train()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hoIzLb32n5vM"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfkZI3z600IP"
   },
   "outputs": [],
   "source": [
    "def createModel(model_name):\n",
    "  needs_out = False\n",
    "  if model_name=='unet':\n",
    "    model = createUnet()\n",
    "  elif model_name=='unetplusplus':\n",
    "    model = createUnetPlusPlus()\n",
    "  elif model_name=='attunet':\n",
    "    model = createUnetAtt()\n",
    "  elif model_name=='transunet':\n",
    "    model = createTransUnet()\n",
    "  elif model_name=='dstransunet':\n",
    "    model = createDSTransUnet()\n",
    "  else:\n",
    "    raise ValueError('no such model')\n",
    "  return model, needs_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "56VewwwtZpa_"
   },
   "outputs": [],
   "source": [
    "path_to_dataset = '/content/drive/MyDrive/datasets/masks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8g1JrvWQqyNk"
   },
   "outputs": [],
   "source": [
    "model_names = [  'unet', \"unetplusplus\", 'attunet', 'transunet', 'dstransunet']\n",
    "# model_names = [ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogMe0mXjkHYN"
   },
   "outputs": [],
   "source": [
    "ready = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "azbPdaLmSEmA",
    "outputId": "b18e6245-2fac-4652-e499-cb43c7e73042"
   },
   "outputs": [],
   "source": [
    "ready"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "V0nacPO5YGpY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# import os\n",
    "# import random\n",
    "# os.mkdir('/content/drive/MyDrive/datasets/test')\n",
    "# os.mkdir('/content/drive/MyDrive/datasets/test/image')\n",
    "# os.mkdir('/content/drive/MyDrive/datasets/test/label')"
   ],
   "metadata": {
    "id": "CqvXPuVTK3Jk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# for _ in range(10):\n",
    "#   files = os.listdir(\"/content/drive/MyDrive/datasets/masks/image\")\n",
    "#   f = random.choice(files)\n",
    "#   print(f)\n",
    "#   os.rename(f\"/content/drive/MyDrive/datasets/masks/image/{f}\", f\"/content/drive/MyDrive/datasets/masks/test/image/{f}\")\n",
    "#   os.rename(f\"/content/drive/MyDrive/datasets/masks/label/{f}\", f\"/content/drive/MyDrive/datasets/masks/test/label/{f}\")"
   ],
   "metadata": {
    "id": "PZlTb_vrJqDn"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "946cf8cd41c1447e8e2a223fd25b418b",
      "7c7a49e85c6b4699a2763bccb6a3e75b",
      "2f25383410354f69897c47636761e227",
      "80c6f674d75d487f8c43841d350f0bd2",
      "c850190636354f0eb862068f70f72dad",
      "009843f7bdbb4eec8ad1a0fef8cd2178",
      "3107b29c606b48c5a8118bdaf9d7604b",
      "ac1cb846c1824acc925ba175dd82d761"
     ]
    },
    "id": "i-QBH4Xgr6m4",
    "outputId": "db9e7b17-0d80-4c67-92e3-8986d4fe5a1a"
   },
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "loss = 'BCE'\n",
    "th = 0.6\n",
    "opt = \"SGD\"\n",
    "for model_name in model_names:\n",
    "  print(model_name)\n",
    "  # training\n",
    "  model, needs_out = createModel(model_name)\n",
    "  update_wb(model_name=model_name, lr=lr, loss=loss, th=th, opt = opt)\n",
    "  path_to_exp = f'/content/drive/MyDrive/Froth/exp_{loss}_{lr}_{opt}'\n",
    "\n",
    "  main(path_to_dataset, path_to_exp, 50, 2, model, model_name, lr=lr, needs_out=needs_out, threshold=th)\n",
    "  ready.append(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGb-QsfPmERX"
   },
   "source": [
    "# Compute & visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pSar_qmWmDV7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VOlKApzn61v"
   },
   "outputs": [],
   "source": [
    "def prepare_data(img_number):\n",
    "    image = Image.open(f'{path_to_dataset}/image/{img_number}.png').convert(\"L\")\n",
    "    w, h = image.size\n",
    "    image = np.array(ImageOps.autocontrast(image, cutoff=3))\n",
    "    # source = np.array(image.crop((0, 0, h, h)))\n",
    "    resized_img = cv2.resize(image.copy(), (256, 256)).reshape(1, 1, 256, 256)\n",
    "\n",
    "    target = cv2.imread(f'{path_to_dataset}/label/{img_number}.png')\n",
    "    target = cv2.cvtColor(target, cv2.COLOR_BGR2GRAY)\n",
    "    h, w = target.shape\n",
    "    # target = target[0:h, 0:h]\n",
    "    resized_target = cv2.resize(target.copy()//255, (256, 256))\n",
    "    return image, resized_img, target, resized_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wqwbgul3mPWN"
   },
   "outputs": [],
   "source": [
    "def compute_result(model, model_name, source, th=0.5):\n",
    "    with torch.no_grad():\n",
    "        a = model(torch.from_numpy(source).type(torch.cuda.FloatTensor)/255)\n",
    "    if model_name=='deeplabv3':\n",
    "      pred_raw= (a['out'].cpu().detach().numpy()[0][0]*255).astype(np.uint8)\n",
    "      pred = (a['out'].cpu().detach().numpy()[0][0]>th).astype(np.uint8)\n",
    "    else:\n",
    "       pred_raw = (a.cpu().detach().numpy()[0][0]*255).astype(np.uint8)\n",
    "       pred = (a.cpu().detach().numpy()[0][0]>th).astype(np.uint8)\n",
    "    return pred_raw, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkqP8qzmsJ11"
   },
   "outputs": [],
   "source": [
    "def visualize(source, target, pred_raw, pred, cont):\n",
    "    plt.figure(figsize=(20,20));\n",
    "    plt.subplot(141);\n",
    "    plt.imshow(source, );\n",
    "    plt.title('Image')\n",
    "    plt.axis('off');\n",
    "    plt.subplot(142);\n",
    "    plt.imshow(target,  cmap='gray');\n",
    "    plt.title('Ground Truth')\n",
    "    plt.axis('off');\n",
    "    plt.subplot(143);\n",
    "    plt.imshow(pred, cmap='gray');\n",
    "    plt.title('Segmentation Output')\n",
    "    plt.axis('off');\n",
    "    plt.subplot(144);\n",
    "    # plt.imshow(pred_raw, cmap='gray');\n",
    "    # plt.title('Segmentation Output')\n",
    "    # plt.axis('off');\n",
    "    plt.imshow(cont);\n",
    "    plt.title('Segmentation Output')\n",
    "    plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EY-EEqkJ8XT"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXRvbVPR_jxt"
   },
   "outputs": [],
   "source": [
    "ready = [ 'unetplusplus', 'unet', 'attunet', 'transunet', 'dstransunet']\n",
    "ths = {\n",
    "    'unet':0.8, 'unetplusplus':0.8, 'attunet':0.8, 'transunet':0.8, 'dstransunet':0.7\n",
    "\n",
    "}\n",
    "# ready = [ 'dstransunet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wxAtuFjAtGb"
   },
   "outputs": [],
   "source": [
    "path_to_exp = \"/content/drive/MyDrive/Froth/exp_BCE_0.0001_ad_lr\"\n",
    "# path_to_exp = \"/content/drive/MyDrive/Froth/exp_DICE_0.0001\"\n",
    "path_to_exp = '/content/drive/MyDrive/Froth/exp_BCE_0.0001'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otqEOwtxd6oB"
   },
   "outputs": [],
   "source": [
    "path_to_dataset = \"/content/drive/MyDrive/datasets/masks/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyRmJYCURQPh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#278, 175"
   ],
   "metadata": {
    "id": "P5QATId4HWOc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ADBbc5Uw73X"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "lr = 1e-6\n",
    "loss = 'MSE'\n",
    "f = random.choice(os.listdir(path_to_dataset+'/image'))\n",
    "for model_name in ready:\n",
    "  # training\n",
    "  print(f)\n",
    "  filename = path_to_exp + f'/{model_name}_weights.pt'\n",
    "  # try:\n",
    "  model = torch.load(filename)\n",
    "  num = int(f.split('.')[0])\n",
    "  model.eval()\n",
    "  source, resized_img, target, resized_target = prepare_data(num)\n",
    "  time_1 = time.time()\n",
    "  pred_raw, pred = compute_result(model, model_name, resized_img, th=ths[model_name])\n",
    "  print(model_name, time.time() - time_1)\n",
    "  print(iou(pred, resized_target))\n",
    "\n",
    "  contours = source.copy()\n",
    "  contours = cv2.cvtColor(contours, cv2.COLOR_GRAY2RGB)\n",
    "  pred = cv2.resize(pred, (contours.shape[1], contours.shape[0]))\n",
    "  cnts, hierarchy = cv2.findContours(pred, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "  cv2.drawContours(contours, cnts, -1, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "  visualize(cv2.cvtColor(source, cv2.COLOR_BGR2RGB),  cv2.resize(target, (contours.shape[1], contours.shape[0])), pred_raw, pred, contours)\n",
    "\n",
    "  # except Exception as e:\n",
    "  #   print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeTPfy9-SlbE"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import random\n",
    "# for _ in range(10):\n",
    "#   files = os.listdir(\"/content/drive/MyDrive/datasets/masks/image\")\n",
    "#   f = random.choice(files)\n",
    "#   print(f)\n",
    "#   os.rename(f\"/content/drive/MyDrive/datasets/masks/image/{f}\", f\"/content/drive/MyDrive/datasets/masks/test/image/{f}\")\n",
    "#   os.rename(f\"/content/drive/MyDrive/datasets/masks/label/{f}\", f\"/content/drive/MyDrive/datasets/masks/test/label/{f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l86wjl5YGGr4"
   },
   "outputs": [],
   "source": [
    "model = torch.load('/content/drive/MyDrive/Froth/froth_dataset/exp/unet_weights.pt')\n",
    "model_name = \"unet\"\n",
    "model.eval()\n",
    "source, resized_img, target, resized_target = prepare_data(5)\n",
    "time_1 = time.time()\n",
    "pred_raw, pred = compute_result(model, model_name, resized_img, th=ths[model_name])\n",
    "visualize(source, target, pred_raw, pred)\n",
    "print(model_name, time.time() - time_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agiRX95KeL3t"
   },
   "source": [
    "# ToDo\n",
    "\n",
    "## Networks\n",
    "###Segmentation\n",
    "\n",
    "#### DeepLab based\n",
    "* DeepLabv3 v (29.10)\n",
    "* DeepLabV3+ \n",
    "* PointRend\n",
    "* Detectron2\n",
    "\n",
    "\n",
    "#### Unet-like\n",
    "* Unet   (29.10) v \n",
    "* Unet++  (30.10) v\n",
    "* Tiny-UNET  (30.10) \n",
    "* PSPnet (30.10) v\n",
    "\n",
    "#### Transformers \n",
    "* HRNet OCR\n",
    "\n",
    "#### Others\n",
    "* FPN  v\n",
    "* MA-Net v\n",
    "* ResT\n",
    "* Mask-RCNN\n",
    "\n",
    "\n",
    "* PointNet\n",
    "* FCN\n",
    "\n",
    "* SFNet (ECCV 2020)\n",
    "* SegFormer (ArXiv 2021)\n",
    "* FaPN (ICCV 2021)\n",
    "* CondNet (IEEE SPL 2021)\n",
    "### Detection\n",
    "*   YOLO\n",
    "*   SSD\n",
    "* RCNN\n",
    "\n",
    "## Losses\n",
    "* MSE +\n",
    "* Cross Entropy Loss\n",
    "* Focal Loss\n",
    "* Dice Loss\n",
    "\n",
    "## OF \n",
    "* FlowNet\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Ow8sv1fJO-lv",
    "x_DkdXswqorB",
    "agiRX95KeL3t"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "946cf8cd41c1447e8e2a223fd25b418b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "VBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7c7a49e85c6b4699a2763bccb6a3e75b",
       "IPY_MODEL_2f25383410354f69897c47636761e227"
      ],
      "layout": "IPY_MODEL_80c6f674d75d487f8c43841d350f0bd2"
     }
    },
    "7c7a49e85c6b4699a2763bccb6a3e75b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "LabelModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c850190636354f0eb862068f70f72dad",
      "placeholder": "​",
      "style": "IPY_MODEL_009843f7bdbb4eec8ad1a0fef8cd2178",
      "value": "0.288 MB of 0.288 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "2f25383410354f69897c47636761e227": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3107b29c606b48c5a8118bdaf9d7604b",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ac1cb846c1824acc925ba175dd82d761",
      "value": 1
     }
    },
    "80c6f674d75d487f8c43841d350f0bd2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c850190636354f0eb862068f70f72dad": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "009843f7bdbb4eec8ad1a0fef8cd2178": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3107b29c606b48c5a8118bdaf9d7604b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac1cb846c1824acc925ba175dd82d761": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
